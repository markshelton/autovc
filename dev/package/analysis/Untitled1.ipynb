{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logManager - INFO - flatten_file | Started\n",
      "logManager - INFO - flatten | Started\n",
      "logManager - ERROR - flatten | Failed | 89.98\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 257, in flatten\n",
      "    sm.execute_script(database_file, script_file)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\sqlManager.py\", line 74, in execute_script\n",
      "    conn.executescript(script)\n",
      "sqlite3.OperationalError: no such column: event_relationships.entity_uuid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - flatten | Failed | 89.98\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 257, in flatten\n",
      "    sm.execute_script(database_file, script_file)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\sqlManager.py\", line 74, in execute_script\n",
      "    conn.executescript(script)\n",
      "sqlite3.OperationalError: no such column: event_relationships.entity_uuid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbLoader - INFO - C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/raw.csv | Already deleted\n",
      "dbLoader - INFO - temp.csv | Export started\n",
      "logManager - ERROR - flatten_file | Failed | 90.01\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 262, in flatten_file\n",
      "    db.export_file(database_file, export_file, file_name)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\dbLoader.py\", line 94, in export_file\n",
      "    try: odo.odo(table_uri, export_file)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\odo.py\", line 91, in odo\n",
      "    return into(target, source, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\multipledispatch\\dispatcher.py\", line 164, in __call__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\into.py\", line 43, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\into.py\", line 149, in into_string_string\n",
      "    return into(a, resource(b, **kwargs), **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\regex.py\", line 91, in __call__\n",
      "    return self.dispatch(s)(s, *args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\resource.py\", line 106, in resource_split\n",
      "    return resource(uri, other, *args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\regex.py\", line 91, in __call__\n",
      "    return self.dispatch(s)(s, *args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\backends\\sql.py\", line 578, in resource_sql\n",
      "    raise ValueError(\"Table does not exist and no dshape provided\")\n",
      "ValueError: Table does not exist and no dshape provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - flatten_file | Failed | 90.01\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 262, in flatten_file\n",
      "    db.export_file(database_file, export_file, file_name)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\dbLoader.py\", line 94, in export_file\n",
      "    try: odo.odo(table_uri, export_file)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\odo.py\", line 91, in odo\n",
      "    return into(target, source, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\multipledispatch\\dispatcher.py\", line 164, in __call__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\into.py\", line 43, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\into.py\", line 149, in into_string_string\n",
      "    return into(a, resource(b, **kwargs), **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\regex.py\", line 91, in __call__\n",
      "    return self.dispatch(s)(s, *args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\resource.py\", line 106, in resource_split\n",
      "    return resource(uri, other, *args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\regex.py\", line 91, in __call__\n",
      "    return self.dispatch(s)(s, *args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\odo\\backends\\sql.py\", line 578, in resource_sql\n",
      "    raise ValueError(\"Table does not exist and no dshape provided\")\n",
      "ValueError: Table does not exist and no dshape provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logManager - INFO - clean_file | Started\n",
      "logManager - INFO - read | Started\n",
      "logManager - ERROR - read | Failed | 0.00\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 53, in read\n",
      "    if nrows is None: df = pd.read_csv(file,low_memory=False)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 646, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 389, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 730, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 923, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1390, in __init__\n",
      "    self._reader = _parser.TextReader(src, **kwds)\n",
      "  File \"pandas\\parser.pyx\", line 373, in pandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\n",
      "  File \"pandas\\parser.pyx\", line 667, in pandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\n",
      "FileNotFoundError: File b'C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/raw.csv' does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - read | Failed | 0.00\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 53, in read\n",
      "    if nrows is None: df = pd.read_csv(file,low_memory=False)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 646, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 389, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 730, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 923, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1390, in __init__\n",
      "    self._reader = _parser.TextReader(src, **kwds)\n",
      "  File \"pandas\\parser.pyx\", line 373, in pandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\n",
      "  File \"pandas\\parser.pyx\", line 667, in pandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\n",
      "FileNotFoundError: File b'C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/raw.csv' does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logManager - INFO - clean | Started\n",
      "logManager - ERROR - clean | Failed | 0.08\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 240, in clean\n",
      "    for column in df:\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - clean | Failed | 0.08\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 240, in clean\n",
      "    for column in df:\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - clean_file | Failed | 0.10\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 268, in clean_file\n",
      "    df.to_csv(clean_file, mode=\"w+\", index=False)\n",
      "AttributeError: 'NoneType' object has no attribute 'to_csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - clean_file | Failed | 0.10\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 268, in clean_file\n",
      "    df.to_csv(clean_file, mode=\"w+\", index=False)\n",
      "AttributeError: 'NoneType' object has no attribute 'to_csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logManager - INFO - load_file | Started\n",
      "logManager - ERROR - load_file | Failed | 0.00\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 273, in load_file\n",
      "    else: df = pd.read_csv(clean_file, encoding=\"latin1\")\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 646, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 389, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 730, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 923, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1390, in __init__\n",
      "    self._reader = _parser.TextReader(src, **kwds)\n",
      "  File \"pandas\\parser.pyx\", line 373, in pandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\n",
      "  File \"pandas\\parser.pyx\", line 667, in pandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\n",
      "FileNotFoundError: File b'C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/clean.csv' does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - load_file | Failed | 0.00\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 273, in load_file\n",
      "    else: df = pd.read_csv(clean_file, encoding=\"latin1\")\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 646, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 389, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 730, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 923, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1390, in __init__\n",
      "    self._reader = _parser.TextReader(src, **kwds)\n",
      "  File \"pandas\\parser.pyx\", line 373, in pandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\n",
      "  File \"pandas\\parser.pyx\", line 667, in pandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\n",
      "FileNotFoundError: File b'C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/clean.csv' does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logManager - INFO - export_dataframe | Started\n",
      "logManager - ERROR - export_dataframe | Failed | 0.01\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\", line 257, in read_sql_table\n",
      "    meta.reflect(only=[table_name], views=True)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\schema.py\", line 3650, in reflect\n",
      "    (bind.engine.url, s, ', '.join(missing)))\n",
      "sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in sqlite:///C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/output.db: (temp)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 287, in export_dataframe\n",
      "    else: df = pd.read_sql_table(table, conn)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\", line 259, in read_sql_table\n",
      "    raise ValueError(\"Table %s not found\" % table_name)\n",
      "ValueError: Table temp not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logManager - ERROR - export_dataframe | Failed | 0.01\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\", line 257, in read_sql_table\n",
      "    meta.reflect(only=[table_name], views=True)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\schema.py\", line 3650, in reflect\n",
      "    (bind.engine.url, s, ', '.join(missing)))\n",
      "sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in sqlite:///C:/Users/mark/Documents/GitHub/honours/dev/package/analysis/output/temp/output.db: (temp)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\logManager.py\", line 58, in wrapper\n",
      "    try: result = f(*args, **kwargs)\n",
      "  File \"C:/Users/mark/Documents/GitHub/honours/dev/package\\analysis\\dataPreparer.py\", line 287, in export_dataframe\n",
      "    else: df = pd.read_sql_table(table, conn)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\", line 259, in read_sql_table\n",
      "    raise ValueError(\"Table %s not found\" % table_name)\n",
      "ValueError: Table temp not found\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-995d7094b642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_flat_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"temp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"temp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mdf_backup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "#setup path\n",
    "path = 'C:/Users/mark/Documents/GitHub/honours/dev/package/'\n",
    "import sys; sys.path.append(path)\n",
    "\n",
    "#standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from collections import OrderedDict\n",
    "\n",
    "#configure display modes\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 20,6\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#load data first time from database\n",
    "import analysis.dataPreparer as dp\n",
    "\n",
    "input_path = path+\"analysis/input/test - Copy.db\"\n",
    "flatten_config = path+\"analysis/config/master_feature.sql\"\n",
    "raw_flat_file = path+\"analysis/output/temp/raw.csv\"\n",
    "clean_flat_file = path+\"analysis/output/temp/clean.csv\"\n",
    "output_path = path+\"analysis/output/temp/output.db\"\n",
    "\n",
    "dp.flatten_file(input_path, flatten_config, raw_flat_file, \"temp\")\n",
    "dp.clean_file(raw_flat_file, clean_flat_file)\n",
    "dp.load_file(output_path, clean_flat_file, \"temp\")\n",
    "df = dp.export_dataframe(output_path, \"temp\")\n",
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_backup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-204dd50278aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#reload data from memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_backup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_backup' is not defined"
     ]
    }
   ],
   "source": [
    "#reload data from memory\n",
    "df = df_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create label\n",
    "label = \"outcome_exit_bool\"\n",
    "y = df[label]\n",
    "\n",
    "#create features\n",
    "df = df.select_dtypes(['number'])\n",
    "drops = [col for col in list(df) if col.startswith((\"key\",\"from\",\"outcome\",\"index\"))]\n",
    "X = df.drop(drops, axis=1)\n",
    "features = list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create lifecycle stages - 2013\n",
    "df[\"keys_company_stage_seed\"] = df[\"confidence_validation_funding_round_codes_list_seed\"]\n",
    "df[\"keys_company_stage_series-a\"] = df[\"confidence_validation_funding_round_codes_list_a\"]\n",
    "df[\"keys_company_stage_series-b\"] = df[\"confidence_validation_funding_round_codes_list_b\"]\n",
    "df[\"keys_company_stage_series-c+\"] = df[\"confidence_validation_funding_round_types_list_series-c+\"]\n",
    "df[\"keys_company_stage_ipo\"] = df[\"confidence_performance_other_ipo_bool\"]\n",
    "df[\"keys_company_stage_acquired\"] = df[\"confidence_performance_other_acquired_bool\"]\n",
    "df[\"keys_company_stage_closed\"] = df[\"confidence_performance_other_closed_bool\"]\n",
    "\n",
    "def label_stage(row):\n",
    "    if row[\"keys_company_stage_closed\"] >= 1: stage = \"Closed\"\n",
    "    elif row[\"keys_company_stage_acquired\"] >= 1: stage = \"Acquired\"\n",
    "    elif row[\"keys_company_stage_ipo\"] >= 1: stage = \"IPO\"\n",
    "    elif row[\"keys_company_stage_series-c+\"] >= 1: stage = \"Series C+\"\n",
    "    elif row[\"keys_company_stage_series-b\"] >= 1: stage = \"Series B\"\n",
    "    elif row[\"keys_company_stage_series-a\"] >= 1: stage = \"Series A\"\n",
    "    elif row[\"keys_company_stage_seed\"] >= 1: stage = \"Seed\"\n",
    "    elif row[\"confidence_validation_funding_rounds_number\"] == 0: stage = \"None\"\n",
    "    else: stage = \"Other\"\n",
    "    return stage\n",
    "\n",
    "df[\"keys_company_stage\"] = df.apply(lambda row: label_stage(row), axis=1)\n",
    "company_stage = df[\"keys_company_stage\"]\n",
    "\n",
    "#Decompose Other and None stages\n",
    "from sklearn.mixture import GaussianMixture\n",
    "mix = GaussianMixture(n_components=2)\n",
    "\n",
    "def label_stage_other(row, stage):\n",
    "    if row[\"keys_company_stage_\"+stage.lower()+\"_young\"] == 1: val = stage + \"- Young\"\n",
    "    elif row[\"keys_company_stage_\"+stage.lower()+\"_young\"] == 0: val = stage + \"- Mature\"\n",
    "    else: val = row[\"keys_company_stage\"]\n",
    "    return val\n",
    "\n",
    "founded_date = df[\"confidence_context_broader_founded_date\"]\n",
    "\n",
    "for stage in [\"Other\", \"None\"]:\n",
    "    founded_date_other = founded_date.loc[df['keys_company_stage'] == stage].dropna()\n",
    "    founded_date_other_stacked = np.vstack(founded_date_other)\n",
    "    mix.fit(founded_date_other_stacked)\n",
    "    pred = mix.predict(founded_date_other_stacked)\n",
    "    max_index = list(mix.means_).index(max(mix.means_))\n",
    "    if max_index == 0: pred = [0 if x==1 else 1 for x in pred]\n",
    "    df[\"keys_company_stage_\"+stage.lower()+\"_young\"] = pd.Series(pred, index=founded_date_other.index)\n",
    "    df[\"keys_company_stage\"] = df.apply(lambda row: label_stage_other(row, stage), axis=1)\n",
    "    df[\"keys_company_stage_\"+stage.lower()+\"_mature\"] = df.apply(lambda row: 1 if row[\"keys_company_stage\"] == stage + \"- Mature\" else 0, axis=1)\n",
    "    \n",
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reload data from memory\n",
    "df = df_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.1 Companies grouped by lifecycle stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8f916cb18841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"survival_date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"confidence_context_broader_closing_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"keys_updated_at_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"keys_company_age\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"survival_date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"confidence_context_broader_founded_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m24\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m365.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO - FIX Company Stage\n",
    "\n",
    "df[\"survival_date\"] = min(df[\"confidence_context_broader_closing_date\"], df[\"keys_updated_at_date\"])\n",
    "df[\"keys_company_age\"] = (df[\"survival_date\"] - df[\"confidence_context_broader_founded_date\"]) / 60 / 60 / 24 / 365.25\n",
    "\n",
    "factors = dict(\n",
    "    Included = [\"Pre-Seed\", \"Angel\", \"Seed\", \"Series A\", \"Series B\", \"Series C+\"],\n",
    "    Excluded = [\"Closed\", \"Acquired\", \"IPO\", \"Non-Startup\"],\n",
    "    Other = [\"None\", \"Other\"])\n",
    "\n",
    "f = {v:k for k,v in {v:k for k,v in factors.items()}}\n",
    "\n",
    "df[\"keys_company_stage_group\"] = df[\"keys_company_stage\"].map(factors)\n",
    "\n",
    "\n",
    "groups = df[label, \"keys_company_stage_group\", \"keys_company_stage\"]\n",
    "columns = [\"keys_company_stage\", \"keys_company_age\", \"confidence_validation_funding_total_value_number\", \"confidence_validation_funding_rounds_number\"]\n",
    "colnames = [\"Observations\", \"Age (Years)\", \"Funding Raised (USD)\", \"Funding Rounds\"]\n",
    "colfuncs = [np.sum, [np.median, stats.iqr], [np.median, stats.iqr], [np.median, stats.iqr]]\n",
    "\n",
    "namefunc = OrderedDict(zip(columns, colnames))\n",
    "aggfunc = OrderedDict(zip(columns, colfuncs))\n",
    "\n",
    "tab = df[columns].groupby([groups]).describe()\n",
    "#tab = tab.agg(aggfunc)\n",
    "tab = tab.rename(columns=namefunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.1 Company ages by lifecycle stage (KDE plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make constraints\n",
    "df = df.loc[df['keys_company_stage_group'] in [\"Included\", \"Other\"]]\n",
    "\n",
    "#update X and y\n",
    "X = df[features]\n",
    "y = df[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.2 Missing features per observation (histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_by_row = X.isnull().sum(axis=1)\n",
    "g = sns.distplot(missing_by_row, bins=20)\n",
    "g.axes.set_xlim(0,len(list(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.3 Missing observations per feature (histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_by_col =  X.isnull().sum(axis=0)\n",
    "g = sns.distplot(missing_by_col,bins=20)\n",
    "g.axes.set_xlim(0,len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Feature Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.4 Eigenvalues extracted from PCA model (line plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Principal Components Analysis (PCA)\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def screeplot(pca, X_std):\n",
    "    y = np.std(pca.transform(X_std), axis=0)**2\n",
    "    x = np.arange(len(y)) + 1\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.xlabel(\"Component Number\")\n",
    "    plt.axhline(y=1)\n",
    "    return y\n",
    "\n",
    "X_std = pd.DataFrame(scale(X), index=X.index, columns=X.columns)\n",
    "pca = PCA().fit(X_std)\n",
    "eigenvalues = screeplot(pca, X_std)\n",
    "retained = np.where(eigenvalues >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.5 Component loadings on each factor from conceptual framework (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "groups = [x.split(\"_\")[2]) for x in features]\n",
    "\n",
    "loadings_matrix = []\n",
    "for x in retained:\n",
    "    loadings = pca.components_[x]\n",
    "    grp_loadings = defaultdict(int)\n",
    "    for group, loading in zip(groups, loadings):\n",
    "        grp_loadings[group] += loading\n",
    "    loadings_matrix.append(grp_loadings)\n",
    "print(loadings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.6 Inter-correlations of each factor from conceptual framework (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_group(row):\n",
    "    col_groups = defaultdict(int)\n",
    "    for group, val in zip(col_groups, list(row)):\n",
    "        col_groups[group] += val\n",
    "    row = pd.Series(col_groups)\n",
    "    return row\n",
    "\n",
    "X_grp = X_std.apply(lambda row: sum_group(row), axis=1)\n",
    "\n",
    "sns.heatmap(X_grp.corr(method=\"spearman\"), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.2 Classification accuracy metrics for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.7 Receiver operating characteristics for each algorithm (line plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Practical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.8 AUC ROC for different training set sizes (line plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.3 Time profiling by system component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.4 Comparison of 2013 slice from 2016 dataset with 2013 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.5 Features ranked by importance for different time slices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.9 ROC curves for models trained on different time slices (line plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.6 Features ranked by importance for different prediction windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.10 ROC curves for models trained on different prediction windows (line plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.11 Change in company stage over different prediction windows (matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.12 ROC curves for target companies at different lifecycle stages (line plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4.13 AUC ROC for models by lifecyle stage and prediction window (matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.7 Three example company profiles and their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Conceptual Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table C.1 List of included features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Additional Classifier Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table D.1 Hyperparameter gridsearch for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table D.2 Optimal Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure D.1 Optimal Decision Tree model (tree)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
